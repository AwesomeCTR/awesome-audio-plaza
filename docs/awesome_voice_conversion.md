# Awesome Voice Conversion

- [Awesome Voice Conversion](#awesome-voice-conversion)
  - [Papers](#papers)
  - [Projects](#projects)


## Papers
- **VoiceShop: A Unified Speech-to-Speech Framework for Identity-Preserving
  Zero-Shot Voice Editing**, `arXiv, 2404.06674`, [arxiv](http://arxiv.org/abs/2404.06674v2), [pdf](http://arxiv.org/pdf/2404.06674v2.pdf), cication: [**-1**](None)

	 *Philip Anastassiou, Zhenyu Tang, Kainan Peng, Dongya Jia, Jiaxin Li, Ming Tu, Yuping Wang, Yuxuan Wang, Mingbo Ma* · ([voiceshopai.github](https://voiceshopai.github.io/))
- **StreamVoice: Streamable Context-Aware Language Modeling for Real-time
  Zero-Shot Voice Conversion**, `arXiv, 2401.11053`, [arxiv](http://arxiv.org/abs/2401.11053v2), [pdf](http://arxiv.org/pdf/2401.11053v2.pdf), cication: [**-1**](None)

	 *Zhichao Wang, Yuanzhe Chen, Xinsheng Wang, Zhuo Chen, Lei Xie, Yuping Wang, Yuxuan Wang*
- [**GPT-SoVITS**](https://github.com/RVC-Boss/GPT-SoVITS) - RVC-Boss ![Star](https://img.shields.io/github/stars/RVC-Boss/GPT-SoVITS.svg?style=social&label=Star)

	 *1 min voice data can also be used to train a good TTS model! (few shot voice cloning)*
- **CoMoSVC: Consistency Model-based Singing Voice Conversion**, `arXiv, 2401.01792`, [arxiv](http://arxiv.org/abs/2401.01792v1), [pdf](http://arxiv.org/pdf/2401.01792v1.pdf), cication: [**-1**](None)

	 *Yiwen Lu, Zhen Ye, Wei Xue, Xu Tan, Qifeng Liu, Yike Guo* · ([comosvc.github](https://comosvc.github.io/))
- **Leveraging Content-based Features from Multiple Acoustic Models for
  Singing Voice Conversion**, `arXiv, 2310.11160`, [arxiv](http://arxiv.org/abs/2310.11160v1), [pdf](http://arxiv.org/pdf/2310.11160v1.pdf), cication: [**-1**](None)

	 *Xueyao Zhang, Yicheng Gu, Haopeng Chen, Zihao Fang, Lexiao Zou, Liumeng Xue, Zhizheng Wu* · ([zhangxueyao](https://www.zhangxueyao.com/data/MultipleContentsSVC/index.html))
- [**llvc**](https://github.com/koeai/llvc) - koeai ![Star](https://img.shields.io/github/stars/koeai/llvc.svg?style=social&label=Star)
- **Rhythm Modeling for Voice Conversion**, `arXiv, 2307.06040`, [arxiv](http://arxiv.org/abs/2307.06040v1), [pdf](http://arxiv.org/pdf/2307.06040v1.pdf), cication: [**-1**](None)

	 *Benjamin van Niekerk, Marc-André Carbonneau, Herman Kamper*
- **HierVST: Hierarchical Adaptive Zero-shot Voice Style Transfer**, `arXiv, 2307.16171`, [arxiv](http://arxiv.org/abs/2307.16171v1), [pdf](http://arxiv.org/pdf/2307.16171v1.pdf), cication: [**-1**](None)

	 *Sang-Hoon Lee, Ha-Yeong Choi, Hyung-Seok Oh, Seong-Whan Lee*
- **SLMGAN: Exploiting Speech Language Model Representations for
  Unsupervised Zero-Shot Voice Conversion in GANs**, `arXiv, 2307.09435`, [arxiv](http://arxiv.org/abs/2307.09435v1), [pdf](http://arxiv.org/pdf/2307.09435v1.pdf), cication: [**-1**](None)

	 *Yinghao Aaron Li, Cong Han, Nima Mesgarani*
- **Voice Conversion With Just Nearest Neighbors**, `arXiv, 2305.18975`, [arxiv](http://arxiv.org/abs/2305.18975v1), [pdf](http://arxiv.org/pdf/2305.18975v1.pdf), cication: [**-1**](None)

	 *Matthew Baas, Benjamin van Niekerk, Herman Kamper* · ([knn-vc](https://github.com/interspeech2023blind/knn-vc) - interspeech2023blind) ![Star](https://img.shields.io/github/stars/interspeech2023blind/knn-vc.svg?style=social&label=Star) · ([bshall.github](https://bshall.github.io/knn-vc/))

## Projects
- [**Applio**](https://github.com/IAHispano/Applio) - IAHispano ![Star](https://img.shields.io/github/stars/IAHispano/Applio.svg?style=social&label=Star)

	 *VITS-based Voice Conversion focused on simplicity, quality and performance.*
- [**Retrieval-based-Voice-Conversion-WebUI**](https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI) - RVC-Project ![Star](https://img.shields.io/github/stars/RVC-Project/Retrieval-based-Voice-Conversion-WebUI.svg?style=social&label=Star)

	 *Voice data <= 10 mins can also be used to train a good VC model!*

# Voice-Omni

## Papers
- **Talk With Human-like Agents: Empathetic Dialogue Through Perceptible
  Acoustic Reception and Reaction**, `arXiv, 2406.12707`, [arxiv](http://arxiv.org/abs/2406.12707v1), [pdf](http://arxiv.org/pdf/2406.12707v1.pdf), cication: [**-1**](None)

	 *Haoqiu Yan, Yongxin Zhu, Kai Zheng, Bing Liu, Haoyu Cao, Deqiang Jiang, Linli Xu* · ([PerceptiveAgent](https://github.com/Haoqiu-Yan/PerceptiveAgent) - Haoqiu-Yan) ![Star](https://img.shields.io/github/stars/Haoqiu-Yan/PerceptiveAgent.svg?style=social&label=Star)
- **SpiRit-LM: Interleaved Spoken and Written Language Model**, `arXiv, 2402.05755`, [arxiv](http://arxiv.org/abs/2402.05755v1), [pdf](http://arxiv.org/pdf/2402.05755v1.pdf), cication: [**2**](https://scholar.google.com/scholar?cites=7368374930715826231&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Tu Anh Nguyen, Benjamin Muller, Bokai Yu, Marta R. Costa-jussa, Maha Elbayad, Sravya Popuri, Paul-Ambroise Duquenne, Robin Algayres, Ruslan Mavlyutov, Itai Gat*

	 · ([speechbot.github](https://speechbot.github.io/spiritlm/index.html))
- **Speak While You Think: Streaming Speech Synthesis During Text Generation**, `icassp 2024-2024 ieee international conference on acoustics …, 2024`, [arxiv](http://arxiv.org/abs/2309.11210v1), [pdf](http://arxiv.org/pdf/2309.11210v1.pdf), cication: [**1**](https://scholar.google.com/scholar?cites=2989454037175742919&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Avihu Dekel, Slava Shechtman, Raul Fernandez, David Haws, Zvi Kons, Ron Hoory*
- **Spoken Question Answering and Speech Continuation Using
  Spectrogram-Powered LLM**, `arXiv, 2305.15255`, [arxiv](http://arxiv.org/abs/2305.15255v4), [pdf](http://arxiv.org/pdf/2305.15255v4.pdf), cication: [**2**](https://scholar.google.com/scholar?cites=2239314690427158927&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Eliya Nachmani, Alon Levkovitch, Roy Hirsch, Julian Salazar, Chulayuth Asawaroengchai, Soroosh Mariooryad, Ehud Rivlin, RJ Skerry-Ryan, Michelle Tadmor Ramanovich* · ([research](https://research.google/blog/spoken-question-answering-and-speech-continuation-using-a-spectrogram-powered-llm/)) · ([LLAMA1-Test-Set](https://github.com/google-research-datasets/LLAMA1-Test-Set) - google-research-datasets) ![Star](https://img.shields.io/github/stars/google-research-datasets/LLAMA1-Test-Set.svg?style=social&label=Star) · ([michelleramanovich.github](https://michelleramanovich.github.io/spectron/spectron))
- **Text-Free Prosody-Aware Generative Spoken Language Modeling**, `arXiv, 2109.03264`, [arxiv](http://arxiv.org/abs/2109.03264v2), [pdf](http://arxiv.org/pdf/2109.03264v2.pdf), cication: [**86**](https://scholar.google.com/scholar?cites=6860409912627023988&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Eugene Kharitonov, Ann Lee, Adam Polyak, Yossi Adi, Jade Copet, Kushal Lakhotia, Tu-Anh Nguyen, Morgane Rivière, Abdelrahman Mohamed, Emmanuel Dupoux*

## Projects
- [Fetching Title#qz1d](https://0nutation.github.io/SpeechGPT2.github.io/)
- [Fetching Title#ytve](https://speechbot.github.io/)
- [**Desktop_BUD-E**](https://github.com/christophschuhmann/Desktop_BUD-E/tree/main) - christophschuhmann ![Star](https://img.shields.io/github/stars/christophschuhmann/Desktop_BUD-E.svg?style=social&label=Star)

	 *This is a voice assitant to run on you laptop*
- [Call to Build Open Multi-Modal Models for Personal Assistants | LAION](https://laion.ai/notes/open-gpt-4-o/)
- [Building GPT2o — Part 1 : Audio. code, pretrained model, colab notebook | by Srinivas Billa | Jun, 2024 | Medium](https://medium.com/@nivibilla/building-gpt2o-part-1-audio-65b66e193784)

	 · ([build-nanogpt](https://github.com/nivibilla/build-nanogpt/tree/audio) - nivibilla) ![Star](https://img.shields.io/github/stars/nivibilla/build-nanogpt.svg?style=social&label=Star)
- [**natural_voice_assistant**](https://github.com/LAION-AI/natural_voice_assistant) - LAION-AI ![Star](https://img.shields.io/github/stars/LAION-AI/natural_voice_assistant.svg?style=social&label=Star)

	 · ([laion](https://laion.ai/blog/bud-e/))
- [**WhisperFusion**](https://github.com/collabora/WhisperFusion) - collabora ![Star](https://img.shields.io/github/stars/collabora/WhisperFusion.svg?style=social&label=Star)

	 *WhisperFusion builds upon the capabilities of WhisperLive and WhisperSpeech to provide a seamless conversations with an AI.*
- [**natural_voice_assistant**](https://github.com/LAION-AI/natural_voice_assistant) - LAION-AI ![Star](https://img.shields.io/github/stars/LAION-AI/natural_voice_assistant.svg?style=social&label=Star)
- [**pipecat**](https://github.com/pipecat-ai/pipecat) - pipecat-ai ![Star](https://img.shields.io/github/stars/pipecat-ai/pipecat.svg?style=social&label=Star)

	 *Open Source framework for voice and multimodal conversational AI*
- [Audio Text GPT - Google Docs](https://docs.google.com/document/d/1nfngl0lv3XAWLQg5z-euYcE6NGGaoH7BdyoujVf7tEc/edit)
- [**gazelle**](https://github.com/tincans-ai/gazelle/tree/main?tab=readme-ov-file) - tincans-ai ![Star](https://img.shields.io/github/stars/tincans-ai/gazelle.svg?style=social&label=Star)

	 *Joint speech-language model - respond directly to audio!* · ([tincans](https://tincans.ai/slm)) · ([x](https://x.com/hingeloss/status/1765440068452331898)) · ([x](https://x.com/hingeloss/status/1770157984745656743))
## End of Speech 
- [**predictivechat**](https://github.com/yoheinakajima/predictivechat?tab=readme-ov-file) - yoheinakajima ![Star](https://img.shields.io/github/stars/yoheinakajima/predictivechat.svg?style=social&label=Star)

	 *Demo of AI chatbot that predicts user message to generate response quickly.*
- **TurnGPT: a Transformer-based Language Model for Predicting Turn-taking
  in Spoken Dialog**, `arXiv, 2010.10874`, [arxiv](http://arxiv.org/abs/2010.10874v1), [pdf](http://arxiv.org/pdf/2010.10874v1.pdf), cication: [**45**](https://scholar.google.com/scholar?cites=4498969182199038561&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Erik Ekstedt, Gabriel Skantze*


## Tutorials
- [Moshi (@kyutai_labs) is an unpolished e2e *full-duplex* model](https://x.com/JulianSlzr/status/1810303916686577858)
- [SNAC with flattening & reconstruction - YouTube](https://www.youtube.com/watch?v=NwZufAJxmMA&ab_channel=DemocratizingAI)


## Datasets
- https://huggingface.co/datasets/TwoAbove/the-project-gutenberg-open-audiobook-collection
- https://huggingface.co/datasets/EQ4You/Emotional_Speech ( gender labels are broken → “Male” and “female” need to be removed from all the captions, man and Woman need to be replaced with Person )
- https://huggingface.co/datasets/speechcolab/gigaspeech
- https://huggingface.co/datasets/parler-tts/mls-eng-10k-tags_tagged_10k_generated/viewer/default/train
- https://huggingface.co/datasets/krishnakalyan3/emo_300k 
- https://huggingface.co/datasets/mozilla-foundation/common_voice_17_0
- https://huggingface.co/datasets/facebook/multilingual_librispeech 
- https://huggingface.co/datasets/librispeech_asr 
- https://huggingface.co/collections/marianna13/laion-audio-630k-65aff8bbaa335c2842f3a730 
- https://huggingface.co/datasets/blanchon/udio_dataset 


- https://huggingface.co/datasets/ChristophSchuhmann/yt-urls-for-emotional-tts
- https://huggingface.co/datasets/ChristophSchuhmann/docu-clips
- https://huggingface.co/datasets/ChristophSchuhmann/movie-clips

- [collabora/librilight-processed-webdataset at main](https://huggingface.co/datasets/collabora/librilight-processed-webdataset/tree/main)
- [LAION-Audio-630k - a marianna13 Collection](https://huggingface.co/collections/marianna13/laion-audio-630k-65aff8bbaa335c2842f3a730)

### LLM
- [**SlimOrca-Dedup**](https://huggingface.co/datasets/Open-Orca/SlimOrca-Dedup?row=0) - Open-Orca 🤗

## Evaluation
- **SD-Eval: A Benchmark Dataset for Spoken Dialogue Understanding Beyond
  Words**, `arXiv, 2406.13340`, [arxiv](http://arxiv.org/abs/2406.13340v1), [pdf](http://arxiv.org/pdf/2406.13340v1.pdf), cication: [**-1**](None)

	 *Junyi Ao, Yuancheng Wang, Xiaohai Tian, Dekun Chen, Jun Zhang, Lu Lu, Yuxuan Wang, Haizhou Li, Zhizheng Wu* · ([SD-Eval](https://github.com/amphionspace/SD-Eval) - amphionspace) ![Star](https://img.shields.io/github/stars/amphionspace/SD-Eval.svg?style=social&label=Star)
## Products
- [Sindarin — Build your own Conversational Speech AI.](https://www.sindarin.tech/)
- [Introducing Deepgram Aura: Lightning Fast Text-to-Speech for Voice AI Agents | Deepgram](https://deepgram.com/learn/aura-text-to-speech-tts-api-voice-ai-agents-launch)