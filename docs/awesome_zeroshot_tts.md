# Awesome Zero Shot TTS

## Gallery
- [**WhisperSpeech**](https://github.com/collabora/WhisperSpeech) - collabora ![Star](https://img.shields.io/github/stars/collabora/WhisperSpeech.svg?style=social&label=Star)

	 *An Open Source text-to-speech system built by inverting Whisper.* · ([collabora.github](https://collabora.github.io/WhisperSpeech/))
- **VALL-T: Decoder-Only Generative Transducer for Robust and
  Decoding-Controllable Text-to-Speech**, `arXiv, 2401.14321`, [arxiv](http://arxiv.org/abs/2401.14321v4), [pdf](http://arxiv.org/pdf/2401.14321v4.pdf), cication: [**-1**](None)

	 *Chenpeng Du, Yiwei Guo, Hankun Wang, Yifan Yang, Zhikang Niu, Shuai Wang, Hui Zhang, Xie Chen, Kai Yu* · ([cpdu.github](https://cpdu.github.io/vallt/))
- **ELLA-V: Stable Neural Codec Language Modeling with Alignment-guided
  Sequence Reordering**, `arXiv, 2401.07333`, [arxiv](http://arxiv.org/abs/2401.07333v1), [pdf](http://arxiv.org/pdf/2401.07333v1.pdf), cication: [**-1**](None)

	 *Yakun Song, Zhuo Chen, Xiaofei Wang, Ziyang Ma, Xie Chen* · ([ereboas.github](https://ereboas.github.io/ELLAV/))
- **OpenVoice: Versatile Instant Voice Cloning**, `arXiv, 2312.01479`, [arxiv](http://arxiv.org/abs/2312.01479v5), [pdf](http://arxiv.org/pdf/2312.01479v5.pdf), cication: [**-1**](None)

	 *Zengyi Qin, Wenliang Zhao, Xumin Yu, Xin Sun* · ([openvoice](https://github.com/myshell-ai/openvoice) - myshell-ai) ![Star](https://img.shields.io/github/stars/myshell-ai/openvoice.svg?style=social&label=Star)
- **HierSpeech++: Bridging the Gap between Semantic and Acoustic
  Representation of Speech by Hierarchical Variational Inference for Zero-shot
  Speech Synthesis**, `arXiv, 2311.12454`, [arxiv](http://arxiv.org/abs/2311.12454v2), [pdf](http://arxiv.org/pdf/2311.12454v2.pdf), cication: [**-1**](None)

	 *Sang-Hoon Lee, Ha-Yeong Choi, Seung-Bin Kim, Seong-Whan Lee* · ([HierSpeechpp](https://github.com/sh-lee-prml/HierSpeechpp) - sh-lee-prml) ![Star](https://img.shields.io/github/stars/sh-lee-prml/HierSpeechpp.svg?style=social&label=Star)
-  [[2308.16692] SpeechTokenizer: Unified Speech Tokenizer for Speech Large Language Models](https://arxiv.org/abs/2308.16692)

	 · ([speechtokenizer](https://github.com/zhangxinfd/speechtokenizer) - zhangxinfd) ![Star](https://img.shields.io/github/stars/zhangxinfd/speechtokenizer.svg?style=social&label=Star)
- [**xtts**](https://huggingface.co/spaces/coqui/xtts) - coqui 🤗

	 · ([huggingface](https://huggingface.co/spaces/coqui/xtts)) · ([tts.readthedocs](https://tts.readthedocs.io/en/dev/models/xtts.html))
- **PromptTTS 2: Describing and Generating Voices with Text Prompt**, `arXiv, 2309.02285`, [arxiv](http://arxiv.org/abs/2309.02285v2), [pdf](http://arxiv.org/pdf/2309.02285v2.pdf), cication: [**3**](https://scholar.google.com/scholar?cites=2491055882531395348&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Yichong Leng, Zhifang Guo, Kai Shen, Xu Tan, Zeqian Ju, Yanqing Liu, Yufei Liu, Dongchao Yang, Leying Zhang, Kaitao Song* · ([speechresearch.github](https://speechresearch.github.io/prompttts2))
- **SpeechX: Neural Codec Language Model as a Versatile Speech Transformer**, `arXiv, 2308.06873`, [arxiv](http://arxiv.org/abs/2308.06873v1), [pdf](http://arxiv.org/pdf/2308.06873v1.pdf), cication: [**10**](https://scholar.google.com/scholar?cites=3146656686281147659&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Xiaofei Wang, Manthan Thakker, Zhuo Chen, Naoyuki Kanda, Sefik Emre Eskimez, Sanyuan Chen, Min Tang, Shujie Liu, Jinyu Li, Takuya Yoshioka*
- **Mega-TTS 2: Zero-Shot Text-to-Speech with Arbitrary Length Speech
  Prompts**, `arXiv, 2307.07218`, [arxiv](http://arxiv.org/abs/2307.07218v2), [pdf](http://arxiv.org/pdf/2307.07218v2.pdf), cication: [**3**](https://scholar.google.com/scholar?cites=16735322993503076322&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Ziyue Jiang, Jinglin Liu, Yi Ren, Jinzheng He, Chen Zhang, Zhenhui Ye, Pengfei Wei, Chunfeng Wang, Xiang Yin, Zejun Ma* · ([mega-tts.github](https://mega-tts.github.io/mega2_demo/))
- [**GPT-SoVITS**](https://github.com/RVC-Boss/GPT-SoVITS?tab=readme-ov-file) - RVC-Boss ![Star](https://img.shields.io/github/stars/RVC-Boss/GPT-SoVITS.svg?style=social&label=Star)

	 *1 min voice data can also be used to train a good TTS model! (few shot voice cloning)*
    - [耗时两个月自主研发的低成本AI音色克隆软件，免费送给大家！【GPT-SoVITS】\_哔哩哔哩\_bilibili](https://www.bilibili.com/video/BV12g4y1m7Uw/?vd_source=1453a06a1e0b377f5c40946333b4423a)
- [**fish-speech**](https://github.com/fishaudio/fish-speech) - fishaudio ![Star](https://img.shields.io/github/stars/fishaudio/fish-speech.svg?style=social&label=Star)

	 *Brand new TTS solution* · ([speech.fish](https://speech.fish.audio/))
- **Pheme: Efficient and Conversational Speech Generation**, `arXiv, 2401.02839`, [arxiv](http://arxiv.org/abs/2401.02839v1), [pdf](http://arxiv.org/pdf/2401.02839v1.pdf), cication: [**-1**](None)

	 *Paweł Budzianowski, Taras Sereda, Tomasz Cichy, Ivan Vulić* · ([arxiv](https://arxiv.org/pdf/2401.02839.pdf)) · ([pheme](https://github.com/PolyAI-LDN/pheme?tab=readme-ov-file) - PolyAI-LDN) ![Star](https://img.shields.io/github/stars/PolyAI-LDN/pheme.svg?style=social&label=Star) · ([polyai-ldn.github](https://polyai-ldn.github.io/pheme/#fn1))
- **A Vector Quantized Approach for Text to Speech Synthesis on Real-World
  Spontaneous Speech**, `arXiv, 2302.04215`, [arxiv](http://arxiv.org/abs/2302.04215v1), [pdf](http://arxiv.org/pdf/2302.04215v1.pdf), cication: [**14**](https://scholar.google.com/scholar?cites=1779712084933914260&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Li-Wei Chen, Shinji Watanabe, Alexander Rudnicky* · ([MQTTS](https://github.com/b04901014/MQTTS) - b04901014) ![Star](https://img.shields.io/github/stars/b04901014/MQTTS.svg?style=social&label=Star)
- [**SC-CNN**](https://github.com/hcy71o/SC-CNN) - hcy71o ![Star](https://img.shields.io/github/stars/hcy71o/SC-CNN.svg?style=social&label=Star)

	 *SC-CNN: Effective Speaker Conditioning Method for Zero-Shot Multi-Speaker Text-to-Speech Systems*
- [SC-CNN-demo: "Effective Speaker Conditioning Method for Zero-Shot Multi-Speaker Text-to-Speech Systems"](https://hcy71o.github.io/SC-CNN-demo/)
- **Transfer Learning Framework for Low-Resource Text-to-Speech using a
  Large-Scale Unlabeled Speech Corpus**, `arXiv, 2203.15447`, [arxiv](http://arxiv.org/abs/2203.15447v2), [pdf](http://arxiv.org/pdf/2203.15447v2.pdf), cication: [**15**](https://scholar.google.com/scholar?cites=9843370604003434067&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Minchan Kim, Myeonghun Jeong, Byoung Jin Choi, Sunghwan Ahn, Joun Yeop Lee, Nam Soo Kim* · ([TransferTTS](https://github.com/hcy71o/TransferTTS) - hcy71o) ![Star](https://img.shields.io/github/stars/hcy71o/TransferTTS.svg?style=social&label=Star) · ([SC-VITS](https://github.com/hcy71o/SC-VITS) - hcy71o) ![Star](https://img.shields.io/github/stars/hcy71o/SC-VITS.svg?style=social&label=Star)

## GenerSpeech
- **GenerSpeech: Towards Style Transfer for Generalizable Out-Of-Domain
  Text-to-Speech**, `arXiv, 2205.07211`, [arxiv](http://arxiv.org/abs/2205.07211v2), [pdf](http://arxiv.org/pdf/2205.07211v2.pdf), cication: [**28**](https://scholar.google.com/scholar?cites=14779318166371088188&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Rongjie Huang, Yi Ren, Jinglin Liu, Chenye Cui, Zhou Zhao* · ([generspeech.github](https://generspeech.github.io/#abstract)) · ([GenerSpeech](https://github.com/Rongjiehuang/GenerSpeech) - Rongjiehuang) ![Star](https://img.shields.io/github/stars/Rongjiehuang/GenerSpeech.svg?style=social&label=Star)

##  [Make-A-Voice](https://make-a-voice.github.io/)
[[2305.19269] Make-A-Voice: Unified Voice Synthesis With Discrete Representation](https://arxiv.org/abs/2305.19269)

![](media/Pasted%20image%2020230609155617.png)

## Mega-TTS
[[2306.03509] Mega-TTS: Zero-Shot Text-to-Speech at Scale with Intrinsic Inductive Bias](https://arxiv.org/abs/2306.03509)

- [Mega-TTS | demo-page](https://mega-tts.github.io/demo-page)

	- speech can be decomposed into several attributes (e.g., content, timbre, prosody, and phase) and each of them should be modeled using a module with appropriate inductive biases. 
	- 
![](media/Pasted%20image%2020230607142456.png)

## SoundStorm
[[2305.09636] SoundStorm: Efficient Parallel Audio Generation](https://arxiv.org/abs/2305.09636)

- [SoundStorm](https://google-research.github.io/seanet/soundstorm/examples/)
- [**soundstorm-pytorch**](https://github.com/lucidrains/soundstorm-pytorch) - lucidrains ![Star](https://img.shields.io/github/stars/lucidrains/soundstorm-pytorch.svg?style=social&label=Star)

	 *Implementation of SoundStorm, Efficient Parallel Audio Generation from Google Deepmind, in Pytorch*

![](media/Pasted%20image%2020230523144528.png)

## NaturalSpeech 2
[[2304.09116] NaturalSpeech 2: Latent Diffusion Models are Natural and Zero-Shot Speech and Singing Synthesizers](https://arxiv.org/abs/2304.09116)

- code: [GitHub - lucidrains/naturalspeech2-pytorch: Implementation of Natural Speech 2, Zero-shot Speech and Singing Synthesizer, in Pytorch](https://github.com/lucidrains/naturalspeech2-pytorch)
- [**Amphion**](https://github.com/open-mmlab/Amphion/tree/main/egs/tts/NaturalSpeech2) - open-mmlab ![Star](https://img.shields.io/github/stars/open-mmlab/Amphion.svg?style=social&label=Star)
- [**NaturalSpeech2**](https://huggingface.co/spaces/amphion/NaturalSpeech2) - amphion 🤗
- [NaturalSpeech 2](https://speechresearch.github.io/naturalspeech2/)
- [微软 NaturalSpeech 2来了，基于扩散模型的语音合成\_哔哩哔哩\_bilibili](https://www.bilibili.com/video/BV1Cc411P7sZ/?vd_source=1453a06a1e0b377f5c40946333b4423a)

![](media%201/Pasted%20image%2020230518150107.png)
![](media%201/Pasted%20image%2020230518150239.png)
![](media%201/Pasted%20image%2020230518150702.png)

## VALL-E X
[[2303.03926] Speak Foreign Languages with Your Own Voice: Cross-Lingual Neural Codec Language Modeling](https://arxiv.org/abs/2303.03926)

- [**VALL-E-X**](https://github.com/Plachtaa/VALL-E-X) - Plachtaa ![Star](https://img.shields.io/github/stars/Plachtaa/VALL-E-X.svg?style=social&label=Star)

	 *An open source implementation of Microsoft's VALL-E X zero-shot TTS model. Demo is available in https://plachtaa.github.io*
- [VALL-E X](https://vallex-demo.github.io/)
![](media%201/Pasted%20image%2020230517185214.png)

![](media/Pasted%20image%2020230522170156.png)

![](media/Pasted%20image%2020230522171334.png)

## SPEAR-TTS
[[2302.03540] Speak, Read and Prompt: High-Fidelity Text-to-Speech with Minimal Supervision](https://arxiv.org/abs/2302.03540)
- [SPEAR-TTS](https://google-research.github.io/seanet/speartts/examples/)
- [**spear-tts-pytorch**](https://github.com/collabora/spear-tts-pytorch) - collabora ![Star](https://img.shields.io/github/stars/collabora/spear-tts-pytorch.svg?style=social&label=Star)

	 *An unofficial PyTorch implementation of SPEAR-TTS.*

![](media%201/Pasted%20image%2020230517185604.png)

![](media/Pasted%20image%2020230523140908.png)

![](media/Pasted%20image%2020230523143403.png)

## VALL-E
[[2301.02111] Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers](https://arxiv.org/abs/2301.02111)

- [VALL-E](https://vall-e.io/)
- [**vall-e**](https://github.com/enhuiz/vall-e) - enhuiz ![Star](https://img.shields.io/github/stars/enhuiz/vall-e.svg?style=social&label=Star)

	 *An unofficial PyTorch implementation of the audio LM VALL-E*

![](media%201/Pasted%20image%2020230517184804.png)

![](media/Pasted%20image%2020230522164202.png)

```
● Beat state-of-the-art TTS system
● Speaker similarity is high
● maintain emotion
● maintain acoustic environment
```

## HierSpeech-c
[HierSpeech: Bridging the Gap between Text and Speech by Hierarchical Variational Inference using Self-supervised Representations for Speech Synthesis | OpenReview](https://openreview.net/forum?id=awdyRVnfQKX)

	 · ([sh-lee-prml.github](https://sh-lee-prml.github.io/hierspeech-demo/)) · ([HierSpeech](https://github.com/CODEJIN/HierSpeech) - CODEJIN) ![Star](https://img.shields.io/github/stars/CODEJIN/HierSpeech.svg?style=social&label=Star)
 
![](media%201/Pasted%20image%2020230518145713.png)

## SNAC-c
[[2211.16866] SNAC: Speaker-normalized affine coupling layer in flow-based architecture for zero-shot multi-speaker text-to-speech](https://arxiv.org/abs/2211.16866)

	 · ([byoungjinchoi.github](https://byoungjinchoi.github.io/snac/))
	 - [GitHub - hcy71o/SNAC](https://github.com/hcy71o/SNAC)

![|500](media%201/Pasted%20image%2020230518145533.png)

## YourTTS-c
[[2112.02418] YourTTS: Towards Zero-Shot Multi-Speaker TTS and Zero-Shot Voice Conversion for everyone](https://arxiv.org/abs/2112.02418)

- [YourTTS](https://edresson.github.io/YourTTS/)

![](media%201/Pasted%20image%2020230518145734.png)

# References
- [**open-tts-tracker**](https://github.com/Vaibhavs10/open-tts-tracker) - Vaibhavs10 ![Star](https://img.shields.io/github/stars/Vaibhavs10/open-tts-tracker.svg?style=social&label=Star)
- [AR-NAR-TTS.pdf](https://tan-xu.github.io/AR-NAR-TTS.pdf)
- [【機器學習2023】語音基石模型 (助教張凱為講授) (1/2) - YouTube](https://www.youtube.com/watch?v=m7Be7ppR6q0&ab_channel=Hung-yiLee)
- [【機器學習2023】語音基石模型 (助教張凱為講授) (2/2) - YouTube](https://www.youtube.com/watch?v=HTAq-CPrU5s&ab_channel=Hung-yiLee)
- [https://speech.ee.ntu.edu.tw/\~hylee/ml/ml2023-course-data/張凱爲-x-機器學習-x-語音基石模型.pdf](https://speech.ee.ntu.edu.tw/~hylee/ml/ml2023-course-data/%E5%BC%B5%E5%87%B1%E7%88%B2-x-%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92-x-%E8%AA%9E%E9%9F%B3%E5%9F%BA%E7%9F%B3%E6%A8%A1%E5%9E%8B.pdf)

- [Fetching Title#0s87](https://www.reddit.com/r/LocalLLaMA/comments/19fegt5/open_tts_tracker/)

# END